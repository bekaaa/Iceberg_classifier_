{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/ggplot/utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "/usr/local/lib/python3.6/site-packages/ggplot/stats/smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "/usr/local/lib64/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from logger import logger\n",
    "import os\n",
    "import cv2\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from ggplot import *\n",
    "#-------------------------------------------------\n",
    "#import tkeras.backend as K\n",
    "#from tkeras.applications import inception_v3, inception_resnet_v2\n",
    "#from tkeras import callbacks as Kcallbacks\n",
    "#from tkeras.utils import plot_model\n",
    "#from tkeras.models import Sequential, Model, load_model\n",
    "#from tkeras.optimizers import Adam\n",
    "#from tkeras.layers import Dense, Conv2D, Input, Activation, ZeroPadding2D, BatchNormalization,\\\n",
    "#    Flatten, MaxPooling2D, Dropout, concatenate, AveragePooling2D\n",
    "#-------------------------------------------------------------------------\n",
    "#from tensorflow import losses as tflosses\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('./data/train/train_dataset.npy')\n",
    "labels = np.load('./data/train/labels.npy')\n",
    "print (train.shape, labels.shape, sep=' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data() :\n",
    "    test_dataset = np.load('./data/test/test_dataset.npy')\n",
    "    test_ids = np.load('./data/test/ids.npy')\n",
    "    print (test_dataset.shape, test_dataset, test_ids, sep=' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_images(imgs, epoches):\n",
    "    \n",
    "    more_images = []\n",
    "    rotated_imgs = []\n",
    "    for e in range(epoches) :\n",
    "        for i in range(0,imgs.shape[0]):\n",
    "            a=imgs[i,:,:,0]\n",
    "            b=imgs[i,:,:,1]\n",
    "            c=imgs[i,:,:,2]\n",
    "            d=imgs[i,:,:,3]\n",
    "            \n",
    "            scale = np.random.random(1) * .6 + 1\n",
    "            M = cv2.getRotationMatrix2D( (a.shape[0]/2,a.shape[1]/2), np.random.randint(0,360), scale)\n",
    "\n",
    "            a1 = cv2.warpAffine(a, M, a.shape)\n",
    "            b1 = cv2.warpAffine(b, M, a.shape)\n",
    "            c1 = cv2.warpAffine(c, M, a.shape)\n",
    "            d1 = cv2.warpAffine(d, M, a.shape)\n",
    "            \n",
    "            rotated_imgs.append(np.dstack((a1,b1,c1,d1)))\n",
    "            \n",
    "            \n",
    "    r = np.array(rotated_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,r))\n",
    "    \n",
    "    return more_images\n",
    "\n",
    "e = 4\n",
    "train = get_more_images(train, e)\n",
    "labels = np.concatenate([labels for i in range(e+1)])\n",
    "print (train.shape, labels.shape, sep=' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting validation set\n",
    "train_dataset, valid_dataset, train_labels, valid_labels = train_test_split(train, labels,\n",
    "                                                                            test_size=.0000001,shuffle=True)\n",
    "\n",
    "\n",
    "print (train_dataset.shape, train_labels.shape, sep=' - ')\n",
    "print (valid_dataset.shape, valid_labels.shape, sep=' - ')\n",
    "\n",
    "del train, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15,15]\n",
    "index = np.random.randint(0, train_dataset.shape[0])\n",
    "f, axarr = plt.subplots(1,train_dataset.shape[3])\n",
    "for i in range(train_dataset.shape[3]):\n",
    "    _ = axarr[i].imshow(train_dataset[index,:,:,i])\n",
    "\n",
    "print (\" ** A ship\" if train_labels[index,0] == 1 else \" ** An iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[:,:,:,[0,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(X, layer_name, paddings, convC, convF, convS, poolF, poolS, droprate=.2):\n",
    "    X = keras.layers.ZeroPadding2D(paddings, name='Zero_Padding_'+layer_name)(X)\n",
    "    X = keras.layers.Conv2D(convC, convF, strides=convS, name='Conv2d_'+layer_name)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=3, name='BN_'+layer_name)(X)\n",
    "    X = keras.layers.Activation('relu', name='relu_activ_'+layer_name)(X)\n",
    "    X = keras.layers.MaxPooling2D(poolF, poolS, name='MaxPooling_'+layer_name)(X)\n",
    "    X = keras.layers.Dropout(droprate, name='dropout_'+layer_name)(X)\n",
    "    return X\n",
    "#--------------------------------------------------------\n",
    "def fullyconnected_layer(X, layer_name, dense_units, activ, droprate=.2):\n",
    "    X = keras.layers.Dense(dense_units, activation=activ, name='Dense_'+layer_name)(X)\n",
    "    if layer_name == 'output' : return X\n",
    "    X = keras.layers.BatchNormalization( name='BN_'+layer_name)(X)\n",
    "    X = keras.layers.Dropout(droprate, name='dropout_'+layer_name)(X)\n",
    "    return X\n",
    "#-----------------------------------------------------------------------\n",
    "def get_model(input_shape):\n",
    "    X_input = Input(input_shape, name= 'input')\n",
    "    # layer 1\n",
    "    X = convolution_layer(X_input, 'layer1', 3, 64,  3, 1, 3, 2, .5)\n",
    "    X = convolution_layer(X,       'layer2', 2, 128, 3, 1, 2, 2, .5)\n",
    "    X = convolution_layer(X,       'layer3', 2, 128, 3, 1, 2, 2, .5)\n",
    "    X = convolution_layer(X,       'layer4', 2, 64,  3, 1, 2, 2, .5)\n",
    "    #-*-*-*-*-*-**-*-*-*-*-*-*-*\n",
    "    X = Flatten()(X)\n",
    "    X = fullyconnected_layer(X, 'FC1', 512, 'relu', .5)\n",
    "    X = fullyconnected_layer(X, 'FC2', 256, 'relu', .5)\n",
    "    X = fullyconnected_layer(X, 'FC3', 256, 'relu', .5)\n",
    "    X = fullyconnected_layer(X, 'output', 1, 'sigmoid')\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model\n",
    "model = get_model([75,75,2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.1), loss='binary_crossentropy',metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callbacks = [ \n",
    "    Kcallbacks.EarlyStopping(patience=10, verbose=1, mode='min'),\n",
    "    Kcallbacks.History(),\n",
    "    Kcallbacks.CSVLogger('./tmp/keras/checkpoints/2/logger1.log', append=True),\n",
    "    Kcallbacks.ReduceLROnPlateau(factor=.1, patience=7, verbose=1),\n",
    "    Kcallbacks.ModelCheckpoint('./tmp/keras/checkpoints/2/ckpt-{val_loss:.4f}-{epoch:02d}.hdf5',\n",
    "                               verbose=1, save_best_only=1, mode='min')\n",
    "] \n",
    "\n",
    "model.fit(x=train_dataset, y=train_labels, batch_size=32, epochs=100, shuffle=True, verbose=1, \n",
    "          validation_split=.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_dataset[2500:2700], train_labels[2500:2700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(train_dataset[2500:2700], train_labels[2500:2700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_dataset(model, subid):\n",
    "    test_data, test_ids = prepare_test_data()\n",
    "    test_preds = model.predict(test_data, batch_size=200, verbose=1)\n",
    "    sub_df = pd.DataFrame({'id':test_ids, 'is_iceberg':test_preds.reshape((-1))})\n",
    "    sub_df.to_csv('./tmp/keras/checkpoints/1/submission-{}'.format(subid), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_test_dataset(model, '.0721')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('./tmp/keras/checkpoints/1/ckpt-0.2062-34.hdf5',\n",
    "                        custom_objects={'log_loss':tflosses.log_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, to_file='./model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = Input([75,75,4], name='X_input')\n",
    "model = inception_v3.InceptionV3(input_tensor=X_input, pooling='max', classes=1, weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./data/train/train_ready.csv', train_dataset.reshape(-1,75*75*2), delimiter=',')\n",
    "np.savetxt('./data/train/labels_ready.csv', train_labels, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.contrib.learn.datasets.base.load_csv_with_header('./data/train/labels_ready.csv',\n",
    "                                                                    target_dtype=np.float, features_dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.data.TextLineDataset('./data/train/train_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite = X.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
